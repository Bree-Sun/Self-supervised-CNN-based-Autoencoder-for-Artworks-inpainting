{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet_with_inception_modules.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iEgPjEyCTUcX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import PIL\n",
        "import tqdm\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unified Configuration Dictionary to change all the configurations in the code\n",
        "\n",
        "CONFIG = {'model_type':'Vanilla_UNet_big_bce',\n",
        "          'epochs':30,\n",
        "          'lr':5e-4,\n",
        "          'weight_decay':1e-5,\n",
        "          'batch_size_train':64,\n",
        "          'batch_size_eval':64,\n",
        "          'coding_layer_activation':nn.Sigmoid,\n",
        "          'kl_weights':0.01,\n",
        "          'inception_out_multiplier':1.2, #times input channels count to get output channel count as inception module output\n",
        "          'loss':nn.MSELoss(),\n",
        "          'device':\"cuda\" if torch.cuda.is_available() else 'cpu'}\n",
        "\n",
        "# seed everything for reproducibility \n",
        "def seed_everything(seed=42):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "R2R3qsuPTWFz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_shape(verbose, to_print):\n",
        "  if verbose:\n",
        "    print(to_print)\n",
        "\n",
        "def double_conv_layers(in_channels, out_channels, kernel_size, activation, padding='same', batch_norm=True, coding_layer=False):\n",
        "  '''\n",
        "  Return Double Convolutional layers given the input parameters\n",
        "\n",
        "  in_channels: input channels for the first convolutional layer\n",
        "  out_channels: output channels for the second convolutional layer\n",
        "  kernel_size: kernel size to use for both the layers\n",
        "  activation: activaiton to apply to both the layers, should pass a activation function and not string.\n",
        "  padding: padding to be applied to the inputs, by default no padding.\n",
        "  batch_norm: if True applies nn.BatchNorm2d() after every Convolutional layer.\n",
        "  '''\n",
        "\n",
        "  if batch_norm:\n",
        "    double_conv = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
        "                                nn.BatchNorm2d(out_channels),\n",
        "                                activation(inplace=True),\n",
        "                                nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding),\n",
        "                                nn.BatchNorm2d(out_channels),\n",
        "                                activation(inplace=True) if not coding_layer else CONFIG['coding_layer_activation']())\n",
        "  else:\n",
        "    double_conv = nn.Sequential(\n",
        "                                nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding),\n",
        "                                activation(inplace=True),\n",
        "                                nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding),\n",
        "                                activation(inplace=True) if not coding_layer else CONFIG['coding_layer_activation']())\n",
        "  \n",
        "  return double_conv\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "  '''\n",
        "  Create a layer of Inception Module which were introduced in GoogLeNet, it is termed as \"Convolutional layer on Steroids\" by Aurelian geron in his book\n",
        "  'Hands on ml with scikit learn and tensorflow'\n",
        "  '''\n",
        "  def __init__(self, input_channels, ratios={'c1':0.3, 'c2':0.35, 'c3':0.1, 'c4':0.25}, verbose=False, coding_layer=False):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.verbose = verbose\n",
        "    self.ratios = ratios\n",
        "    self.coding_layer = coding_layer\n",
        "\n",
        "    self.inception_out = int(CONFIG['inception_out_multiplier']*input_channels)\n",
        "\n",
        "    c1_in = int(self.ratios['c1']*input_channels)\n",
        "    c1_out = c1_in\n",
        "\n",
        "    c2_in = int(self.ratios['c2']*input_channels)\n",
        "    c2_out = int(self.ratios['c2']*self.inception_out)\n",
        "\n",
        "    c3_in = int(self.ratios['c3']*input_channels)\n",
        "    c3_out = int(self.ratios['c3']*self.inception_out)\n",
        "\n",
        "    c4_in = int(self.ratios['c4']*input_channels)\n",
        "    c4_out = c4_in\n",
        "\n",
        "    total_in = c1_in + c2_in + c3_in + c4_in\n",
        "\n",
        "    if total_in != input_channels:\n",
        "      c4_out += (input_channels-total_in) # decrease/increase the difference from last channel\n",
        "\n",
        "    total_out = c1_out + c2_out + c3_out +c4_out\n",
        "\n",
        "    if total_out != self.inception_out:\n",
        "      c3_out += (self.inception_out - total_out) \n",
        "\n",
        "    \n",
        "\n",
        "    self.channel_1 = nn.Conv2d(in_channels=input_channels, out_channels=c1_out, kernel_size=1, stride=1, padding='same')\n",
        "\n",
        "    self.channel_2 = nn.Sequential(nn.Conv2d(in_channels=input_channels, out_channels=c2_in, kernel_size=1, stride=1, padding='same'),\n",
        "                              nn.Conv2d(in_channels=c2_in, out_channels=c2_out, kernel_size=3, stride=1, padding='same'))\n",
        "    \n",
        "    self.channel_3 = nn.Sequential(nn.Conv2d(in_channels=input_channels, out_channels=c3_in, kernel_size=1, stride=1, padding='same'),\n",
        "                              nn.Conv2d(in_channels=c3_in, out_channels=c3_out, kernel_size=5, stride=1, padding='same'))\n",
        "    \n",
        "    self.channel_4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                              nn.Conv2d(in_channels=input_channels, out_channels=c4_out, kernel_size=1, stride=1, padding='same'))\n",
        "    \n",
        "  def forward(self, input):\n",
        "    print_shape(self.verbose, f'input shape : {input.shape}')\n",
        "    x1 = self.channel_1(input)\n",
        "    print_shape(self.verbose, f'Channel 1 : {x1.shape}')\n",
        "    x2 = self.channel_2(input)\n",
        "    print_shape(self.verbose, f'Channel 2 : {x2.shape}')\n",
        "    x3 = self.channel_3(input)\n",
        "    print_shape(self.verbose, f'Channel 3 : {x3.shape}')\n",
        "    x4 = self.channel_4(input)\n",
        "    print_shape(self.verbose, f'Channel 4 : {x4.shape}')\n",
        "    x = CONFIG['coding_layer_activation']()(torch.cat([x1, x2, x3, x4], 1)) if self.coding_layer else torch.cat([x1, x2, x3, x4], 1)\n",
        "    print_shape(self.verbose, f'Final shape : {x.shape}')\n",
        "    return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, \n",
        "               down_conv_out=[64, 128, 256, 512], \n",
        "               down_conv_ks=[3, 3, 3, 3],\n",
        "               down_conv_activation=nn.ReLU,\n",
        "               up_conv_out=[256, 128, 64],\n",
        "               up_conv_ks=[3, 3, 3],\n",
        "               up_conv_activation=nn.ReLU,\n",
        "               pad='same',\n",
        "               add_inception=False,\n",
        "               sparse_encoder=False,\n",
        "               verbose=False):\n",
        "    super().__init__()\n",
        "    \n",
        "\n",
        "    self.down_conv_out = down_conv_out\n",
        "    self.down_conv_ks = down_conv_ks\n",
        "    self.down_conv_activation = down_conv_activation\n",
        "    self.up_conv_out = up_conv_out\n",
        "    self.up_conv_ks = up_conv_ks\n",
        "    self.up_conv_activation = up_conv_activation\n",
        "    self.pad = pad \n",
        "    self.add_inception = add_inception # add inception module or not\n",
        "    self.sparse_encoder = sparse_encoder # add sparsity using KL divergence on encoding layer to create a sparse autoencoder\n",
        "    self.verbose = verbose # False if do not want shape transformations\n",
        "\n",
        "    # Down Conv Layers\n",
        "    self.down_conv1 = double_conv_layers(3, down_conv_out[0], down_conv_ks[0], down_conv_activation, padding=pad)\n",
        "    self.down_conv2 = double_conv_layers(down_conv_out[0], down_conv_out[1], down_conv_ks[1], down_conv_activation, padding=pad)\n",
        "    self.down_conv3 = double_conv_layers(down_conv_out[1], down_conv_out[2], down_conv_ks[2], down_conv_activation, padding=pad)\n",
        "    self.down_conv4 = double_conv_layers(down_conv_out[2], down_conv_out[3], down_conv_ks[3], down_conv_activation, padding=pad, coding_layer=not self.add_inception)\n",
        "\n",
        "    # Inception Modules\n",
        "    inception_in_1 = down_conv_out[3]\n",
        "    inception_in_2 = int(CONFIG['inception_out_multiplier'] * inception_in_1)\n",
        "    inception_in_3 = int(CONFIG['inception_out_multiplier'] * inception_in_2)\n",
        "    self.inception_module_1 = InceptionModule(inception_in_1)\n",
        "    self.inception_module_2 = InceptionModule(inception_in_2)\n",
        "    self.inception_module_3 = InceptionModule(inception_in_3, coding_layer=True)\n",
        "    \n",
        "    # Conv Transpose layers\n",
        "    transpose1_in = int(CONFIG['inception_out_multiplier'] * inception_in_3)\n",
        "    self.up_transpose1 = nn.ConvTranspose2d(transpose1_in, up_conv_out[0], 2, 2) if self.add_inception else nn.ConvTranspose2d(down_conv_out[3], up_conv_out[0], 2, 2)\n",
        "    self.up_transpose2 = nn.ConvTranspose2d(up_conv_out[0], up_conv_out[1], 2, 2)\n",
        "    self.up_transpose3 = nn.ConvTranspose2d(up_conv_out[1], up_conv_out[2], 2, 2)\n",
        "    \n",
        "    # Up Conv Layers\n",
        "    self.up_conv1 = double_conv_layers(down_conv_out[3], up_conv_out[0], up_conv_ks[0], up_conv_activation, padding=pad)\n",
        "    self.up_conv2 = double_conv_layers(up_conv_out[0], up_conv_out[1], up_conv_ks[1], up_conv_activation, padding=pad)\n",
        "    self.up_conv3 = double_conv_layers(up_conv_out[1], up_conv_out[2], up_conv_ks[2], up_conv_activation, padding=pad)\n",
        "\n",
        "    # final output conv\n",
        "    self.output_conv = nn.Conv2d(up_conv_out[2], 3, 1)\n",
        "\n",
        "    # Maxpooling\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Down Conv Encoder Part\n",
        "    print_shape(self.verbose, f'Start : {input.shape}')\n",
        "\n",
        "    x1 = self.down_conv1(input)\n",
        "    print_shape(self.verbose, f'After Down Conv 1 : {x1.shape}')\n",
        "\n",
        "    x = self.maxpool(x1)\n",
        "    print_shape(self.verbose, f'After maxpool : {x.shape}')\n",
        "\n",
        "    x2 = self.down_conv2(x)\n",
        "    print_shape(self.verbose, f'After Down Conv 2 : {x2.shape}')\n",
        "\n",
        "    x = self.maxpool(x2)\n",
        "    print_shape(self.verbose, f'After maxpool : {x.shape}')\n",
        "\n",
        "    x3 = self.down_conv3(x)\n",
        "    print_shape(self.verbose, f'After Down Conv 3 : {x3.shape}')\n",
        "\n",
        "    x = self.maxpool(x3)\n",
        "    print_shape(self.verbose, f'After maxpool : {x.shape}')\n",
        "\n",
        "    encoding = self.down_conv4(x)                  # final encoder output to which we will apply loss for sparsity incase of sparse encoder\n",
        "    print_shape(self.verbose, f'After Down Conv 4 : {encoding.shape}')\n",
        "\n",
        "    if self.add_inception:\n",
        "      x = self.inception_module_1(encoding)\n",
        "      print_shape(self.verbose, f'After 1st Inception module : {x.shape}')\n",
        "\n",
        "      x = self.inception_module_2(x)\n",
        "      print_shape(self.verbose, f'After 2nd Inception module : {x.shape}')\n",
        "\n",
        "      encoding = self.inception_module_3(x)\n",
        "      print_shape(self.verbose, f'After 3rd Inception module : {encoding.shape}')\n",
        "\n",
        "    # Up Conv Decoder Part\n",
        "    x = self.up_transpose1(encoding) \n",
        "    print_shape(self.verbose, f'After Up Transpose 1 : {x.shape}')\n",
        "\n",
        "    x = self.up_conv1(torch.cat([x, x3], 1)) # skip connection from down_conv3\n",
        "    print_shape(self.verbose, f'After Up Conv 1 : {x.shape}')\n",
        "\n",
        "    x = self.up_transpose2(x)\n",
        "    print_shape(self.verbose, f'After Up Transpose 2 : {x.shape}')\n",
        "\n",
        "    x = self.up_conv2(torch.cat([x, x2], 1)) # skip connection from down_conv2\n",
        "    print_shape(self.verbose, f'After Up Conv 2 : {x.shape}')\n",
        "\n",
        "    x = self.up_transpose3(x)\n",
        "    print_shape(self.verbose, f'After Up Transpose 3 : {x.shape}')\n",
        "\n",
        "    x = self.up_conv3(torch.cat([x, x1], 1)) # skip connection from down_conv1\n",
        "    print_shape(self.verbose, f'After Up Conv 3 : {x.shape}')\n",
        "\n",
        "    # final output conv layer\n",
        "    x = self.output_conv(x)\n",
        "    print_shape(self.verbose, f'After Final output conv : {x.shape}')\n",
        "    \n",
        "    if self.sparse_encoder:\n",
        "      return x, encoding\n",
        "\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "D8JARoA3TWDg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inception module test\n",
        "image = torch.zeros(1, 512, 64, 64)\n",
        "im = InceptionModule(512, verbose=True)\n",
        "i = im(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpI2HKmKTWBX",
        "outputId": "4f6435f8-fda2-48b4-f204-9df4ce27adf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape : torch.Size([1, 512, 64, 64])\n",
            "Channel 1 : torch.Size([1, 153, 64, 64])\n",
            "Channel 2 : torch.Size([1, 214, 64, 64])\n",
            "Channel 3 : torch.Size([1, 118, 64, 64])\n",
            "Channel 4 : torch.Size([1, 129, 64, 64])\n",
            "Final shape : torch.Size([1, 614, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with inception modules\n",
        "image = torch.zeros(1, 3, 128, 128)\n",
        "model = UNet(add_inception=True, verbose=True)\n",
        "x = model(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICYVnonWTV-5",
        "outputId": "6fcbb4a0-10a0-4b23-c86b-645db2b8e10c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start : torch.Size([1, 3, 128, 128])\n",
            "After Down Conv 1 : torch.Size([1, 64, 128, 128])\n",
            "After maxpool : torch.Size([1, 64, 64, 64])\n",
            "After Down Conv 2 : torch.Size([1, 128, 64, 64])\n",
            "After maxpool : torch.Size([1, 128, 32, 32])\n",
            "After Down Conv 3 : torch.Size([1, 256, 32, 32])\n",
            "After maxpool : torch.Size([1, 256, 16, 16])\n",
            "After Down Conv 4 : torch.Size([1, 512, 16, 16])\n",
            "After 1st Inception module : torch.Size([1, 614, 16, 16])\n",
            "After 2nd Inception module : torch.Size([1, 736, 16, 16])\n",
            "After 3rd Inception module : torch.Size([1, 883, 16, 16])\n",
            "After Up Transpose 1 : torch.Size([1, 256, 32, 32])\n",
            "After Up Conv 1 : torch.Size([1, 256, 32, 32])\n",
            "After Up Transpose 2 : torch.Size([1, 128, 64, 64])\n",
            "After Up Conv 2 : torch.Size([1, 128, 64, 64])\n",
            "After Up Transpose 3 : torch.Size([1, 64, 128, 128])\n",
            "After Up Conv 3 : torch.Size([1, 64, 128, 128])\n",
            "After Final output conv : torch.Size([1, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6x1QG1k0TV8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gtyg4ZsMTV5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Th5k09A9TV3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JOxIz_WKTV0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gww_OaJ7TVx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}